\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tabularx}
\usepackage[procnames]{listings}
\usepackage{color}
\usepackage{fixltx2e}
 
\begin{document}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}

\title{Lab 1 - Decision Trees}

\author{Martin Pettersson\\martinp4@kth.se}

\date{\today}

\maketitle

\section{Assignment 1}

The file dtree.py defines a function entropy which calculates the entropy of a dataset. Import this file along with the monks datasets and use it to calculate the entropy of the training datasets.

\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    Dataset & Entropy \\ \hline
    MONK-1 & 1.0 \\
    MONK-2 & 0.957117428265 \\
    MONK-3 & 0.999806132805 \\
    \hline
    \end{tabular}
\end{center}

\section{Assignment 2}

Use the function averageGain (defined in dtree.py) to calculate the expected information gain corresponding to each of the six at- tributes. Note that the attributes are represented as instances of the class Attribute (defined in monkdata.py) which you can access via m.attributes[0], ..., m.attributes[5]. \\

\begin{tabularx}{\textwidth}{ |X|X|X|X|X|X|X| }
  \hline
  Dataset & $a_1$ & $a_2$ & $a_3$ & $a_4$ & $a_5$ & $a_6$ \\
  \hline 
  MONK-1  & 0.075273  & 0.005838  & 0.004708 & 0.026312  & 0.287031  & 0.000758  \\
  \hline
  MONK-2  & 0.003756 & 0.002458 & 0.001056 & 0.015664 & 0.017277 & 0.006248 \\
  \hline
  MONK-3  & 0.007121 & 0.293736 & 0.000831 & 0.002892 & 0.255912 & 0.007077  \\
  \hline
\end{tabularx}\\

Based on the results, which attribute should be used for splitting the examples
at the root node?\\
\textit{Answer: } According to wiki, you should choose an attribute with high mutual information, which in this case is $a_5$.

\newpage
\section{Assignment 3}

Compute the train and test set errors for the three Monk datasets for the full trees.

\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
     & $E\textsubscript{train}$ & $E\textsubscript{test}$ \\ \hline
    MONK-1 & 1.0 & 0.828703703704 \\
    MONK-2 & 1.0 & 0.69212962963 \\
    MONK-3 & 1.0 & 0.944444444444 \\
    \hline
    \end{tabular}
\end{center}

\section{Assignment 4}

\begin{lstlisting}
def partition(data, fraction):
    ldata = list(data)
    random.shuffle(ldata)
    breakPoint = int(len(ldata) * fraction)
    return ldata[:breakPoint], ldata[breakPoint:]

monk1train, monk1val = partition(m.monk1, 0.6)

def prune_tree(tree, validation):
    pruned_trees = d.allPruned(tree)
    pruned_trees_performance = [0 for x in range(len(pruned_trees))]
    for candidate in pruned_trees:
        index = pruned_trees.index(candidate)
        pruned_trees_performance[index] = d.check(candidate, validation)
    if d.check(tree, validation) <= max(pruned_trees_performance):
        tree = pruned_trees[pruned_trees_performance.index
        				(max(pruned_trees_performance))]
        tree = prune_tree(tree, validation)
    return tree
\end{lstlisting}

\begin{tabularx}{\textwidth}{ |X|X|X|X|X|X|X| }
  \hline
  Dataset & $f = 0.3$ & $f = 0.4$ & $f = 0.5$ & $f = 0.6$ & $f = 0.7$ & $f = 0.8$ \\
  \hline 
  MONK-1  & 0.8125  & 0.814  & 0.768 & 0.754  & 0.771  & 0.708  \\
  \hline
  MONK-3  & 0.958 & 0.935 & 0.977 & 0.963 & 0.944 & 0.949  \\
  \hline
\end{tabularx}\\


\end{document}
